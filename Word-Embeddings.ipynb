{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import wordnet\n",
    "from collections import OrderedDict\n",
    "from itertools import combinations\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notion of `cosine similarity` has become the standard measure of similarity between elements in vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TODO explain cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To normalize a vector, we shrink all values so they fall between $0$ and $1$. <br>\n",
    "$vector_{normalized} = \\frac{vector}{\\sqrt{vector \\cdot vector}}$ (where $\\cdot$ represents the *dot product*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to normalize a vector\n",
    "def normalize_vector(vector):\n",
    "    \"\"\"\n",
    "    Normalizes a vector so that all its values are between 0 and 1\n",
    "    :param: vector: a `numpy` vector\n",
    "    :return: a normalized `numpy` vector\n",
    "    \"\"\"\n",
    "    # norm = np.sqrt(vector.dot(vector))\n",
    "    # numpy has a built in function\n",
    "    norm = np.linalg.norm(vector)\n",
    "    if norm:\n",
    "        return vector / norm\n",
    "    else:\n",
    "        # if norm == 0, then original vector was all 0s\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original vector [1 2 3]\n",
      "normalized vector [ 0.26726124  0.53452248  0.80178373]\n"
     ]
    }
   ],
   "source": [
    "vector_3d = np.array([1,2,3])\n",
    "print(\"original vector\", vector_3d)\n",
    "print(\"normalized vector\", normalize_vector(vector_3d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing a vector maintains the relationship between the values. <br>\n",
    "$0.267$ is $\\frac{1}{3}$rd of $0.801$ just like $1$ is $\\frac{1}{3}$rd of $3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Cosine similarity` has become the standard metric for determining \"similarity\" of elements in vector space. <br>\n",
    "It is simply the `dot product` of two normalized vectors. <br>\n",
    "$cosSim = vector_{normalized}^a \\cdot vector_{normalized}^b$ <br>\n",
    "This score will be between $0$ (representing absolutely no similarity) and $1$ representing equality. <br>\n",
    "**Note:** `cosine similarity` is a symmetric measurement, so $vector_{normalized}^a \\cdot vector_{normalized}^b = vector_{normalized}^b \\cdot vector_{normalized}^a$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate cosine similarity of two vectors\n",
    "def cos_sim(vector_one, vector_two):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity of two `numpy` vectors\n",
    "    param: vector_one: a `numpy` vector\n",
    "    param: vector_two: a `numpy` vector\n",
    "    return: A score between 0 and 1\n",
    "    \"\"\"\n",
    "    # ensure that both vectors are already normalized\n",
    "    vector_one_norm = normalize_vector(vector_one)\n",
    "    vector_two_norm = normalize_vector(vector_two)\n",
    "    \n",
    "    # calculate the dot product between the two normalized vectors\n",
    "    return vector_one_norm.dot(vector_two_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity of vector_one and vector_two 0.948683298051\n",
      "cosine similarity of vector_one and vector_three 0.904534033733\n",
      "cosine similarity of vector_one and vector_four 0.904534033733\n"
     ]
    }
   ],
   "source": [
    "vector_one = np.array([1,1,1,1,1])\n",
    "vector_two = np.array([1,1,1,1,2])\n",
    "vector_three = np.array([1,2,3,4,5])\n",
    "vector_four = np.array([10,20,30,40,50])\n",
    "print(\"cosine similarity of vector_one and vector_two\", cos_sim(vector_one, vector_two))\n",
    "print(\"cosine similarity of vector_one and vector_three\", cos_sim(vector_one, vector_three))\n",
    "print(\"cosine similarity of vector_one and vector_four\", cos_sim(vector_one, vector_four))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why not just use Euclidean Distance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See http://stackoverflow.com/questions/9314576/calculate-distance-between-two-vectors-of-different-length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate euclidean distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the \"Similarity\" of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a metric to measure \"similarity\" we can use it to calculate (computationally) the \"similarity\" between two words....\n",
    "\n",
    "...as long as we project those words into vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: One-hot vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create an indexed list of your vocabulary\n",
    "2. Generate the \"one-hot\" (vector with only one $1$ in it and the rest $0$s) for any word in your vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = ['apple', 'banana', 'orange', 'cantaloupe', 'peach']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate vocabulary lookup\n",
    "def build_voc_lookup(list_of_voc):\n",
    "    \"\"\"\n",
    "    Generates a dictionary where the key is the word and the value is its index\n",
    "    :param: list_of_voc: list of vocabulary words\n",
    "    :return: Dictionary of vocabulary\n",
    "    \"\"\"\n",
    "    lookup_dict = OrderedDict()\n",
    "    counter = 0\n",
    "    for word in list_of_voc:\n",
    "        lookup_dict[word] = counter\n",
    "        counter+=1\n",
    "    return lookup_dict\n",
    "\n",
    "# lookup word\n",
    "def lookup_word(lookup_dict, word):\n",
    "    \"\"\" \n",
    "    Looks up a given word in the vocabulary dictionary, and returns None if word not in vocabulary\n",
    "    :param: lookup_dict: lookup-dictionary built with build_voc_lookup()\n",
    "    :param: word to index\n",
    "    :returns: index of word in vocabulary or None\n",
    "    \"\"\"\n",
    "    if word in lookup_dict:\n",
    "        return lookup_dict[word]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lookup_dict = build_voc_lookup(vocabulary)\n",
    "print(lookup_word(lookup_dict, 'peach'))\n",
    "print(lookup_word(lookup_dict, 'hashbrown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build one-hot vector for word\n",
    "def make_one_hot(lookup_dict, word):\n",
    "    \"\"\"\n",
    "    Builds a one-hot numpy vector for a word\n",
    "    :param: lookup_dict: lookup-dictionary built with build_voc_lookup()\n",
    "    :param: word: word to convert to one-hot\n",
    "    :returns: numpy vector with dimension equal to size of vocabulary\n",
    "    \"\"\"\n",
    "    # get size of vocabulary\n",
    "    voc_size = len(lookup_dict.items())\n",
    "    # initialize empty vector of zeros with the size of the vocabulary\n",
    "    one_hot = np.zeros((voc_size))\n",
    "    # get index of word (or None if not in vocabulary)\n",
    "    word_index = lookup_word(lookup_dict, word)\n",
    "    # make the nth dimension of one-hot (representing the index of word in vocabulary) to 1\n",
    "    if word_index or word_index == 0:\n",
    "        one_hot[word_index] = 1\n",
    "    # if word not in vocabulary, the one-hot will remain zeros\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot vector for '      apple' [ 1.  0.  0.  0.  0.]\n",
      "one-hot vector for '     banana' [ 0.  1.  0.  0.  0.]\n",
      "one-hot vector for '     orange' [ 0.  0.  1.  0.  0.]\n",
      "one-hot vector for ' cantaloupe' [ 0.  0.  0.  1.  0.]\n",
      "one-hot vector for '      peach' [ 0.  0.  0.  0.  1.]\n",
      "one-hot vector for '  hashbrown' [ 0.  0.  0.  0.  0.]\n",
      "one-hot vector for '    Capizzi' [ 0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "for word in vocabulary + ['hashbrown', 'Capizzi']:\n",
    "    print(\"one-hot vector for '{:>11}'\".format(word), make_one_hot(lookup_dict, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The problem with one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity between apple and banana 0.0\n",
      "cosine similarity between apple and orange 0.0\n",
      "cosine similarity between apple and cantaloupe 0.0\n",
      "cosine similarity between apple and peach 0.0\n",
      "cosine similarity between apple and Capizzi 0.0\n",
      "cosine similarity between apple and Phoenix 0.0\n",
      "cosine similarity between banana and orange 0.0\n",
      "cosine similarity between banana and cantaloupe 0.0\n",
      "cosine similarity between banana and peach 0.0\n",
      "cosine similarity between banana and Capizzi 0.0\n",
      "cosine similarity between banana and Phoenix 0.0\n",
      "cosine similarity between orange and cantaloupe 0.0\n",
      "cosine similarity between orange and peach 0.0\n",
      "cosine similarity between orange and Capizzi 0.0\n",
      "cosine similarity between orange and Phoenix 0.0\n",
      "cosine similarity between cantaloupe and peach 0.0\n",
      "cosine similarity between cantaloupe and Capizzi 0.0\n",
      "cosine similarity between cantaloupe and Phoenix 0.0\n",
      "cosine similarity between peach and Capizzi 0.0\n",
      "cosine similarity between peach and Phoenix 0.0\n",
      "cosine similarity between Capizzi and Phoenix 0.0\n"
     ]
    }
   ],
   "source": [
    "# add two OOV words to vocabulary\n",
    "vocabulary_plus_oov = vocabulary + [\"Capizzi\", \"Phoenix\"]\n",
    "# get all combinations\n",
    "all_combinations = combinations(vocabulary_plus_oov, 2)\n",
    "# iterate through all combinations and calculate cosine similarity\n",
    "for (word1, word2) in all_combinations:\n",
    "    one_hot_word_1 = make_one_hot(lookup_dict, word1)\n",
    "    one_hot_word_2 = make_one_hot(lookup_dict, word2)\n",
    "    print(\"cosine similarity between {} and {}\".format(word1, word2), cos_sim(one_hot_word_1, one_hot_word_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Encode spelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following a similar pattern as the one-hot of a word over a vocabulary, let's build word vectors represented by the frequency of the letters present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphabet = list(string.ascii_lowercase) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# since we don't need to worry about \"out-of-vocabulary\" now, we can just use alphabet.index([letter])\n",
    "def lookup_letter(letter):\n",
    "    return alphabet.index(letter.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0\n",
      "A 0\n"
     ]
    }
   ],
   "source": [
    "print(\"a\", lookup_letter('a'))\n",
    "print(\"A\", lookup_letter('A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_spelling_vector(word):\n",
    "    \"\"\"\n",
    "    Converts a word into a vector of dimension 26 where each cell contains the count for that letter\n",
    "    :param: word: word to vectorize\n",
    "    :returns: numpy vector of 26 dimensions\n",
    "    \"\"\"\n",
    "    # initialize vector with zeros\n",
    "    spelling_vector = np.zeros((26))\n",
    "    # iterate through each letter and update count\n",
    "    for letter in word:\n",
    "        if letter in string.ascii_letters:\n",
    "            letter_index = lookup_letter(letter)\n",
    "            spelling_vector[letter_index] = spelling_vector[letter_index] + 1\n",
    "    return spelling_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "        0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_spelling_vector(\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity between apple and banana 0.303045763366\n",
      "cosine similarity between apple and orange 0.308606699924\n",
      "cosine similarity between apple and cantaloupe 0.654653670708\n",
      "cosine similarity between apple and peach 0.676123403783\n",
      "cosine similarity between apple and Capizzi 0.341881729379\n",
      "cosine similarity between apple and Phoenix 0.428571428571\n",
      "cosine similarity between banana and orange 0.54554472559\n",
      "cosine similarity between banana and cantaloupe 0.617213399848\n",
      "cosine similarity between banana and peach 0.3585685828\n",
      "cosine similarity between banana and Capizzi 0.241746889208\n",
      "cosine similarity between banana and Phoenix 0.20203050891\n",
      "cosine similarity between orange and cantaloupe 0.589255650989\n",
      "cosine similarity between orange and peach 0.36514837167\n",
      "cosine similarity between orange and Capizzi 0.123091490979\n",
      "cosine similarity between orange and Phoenix 0.462910049886\n",
      "cosine similarity between cantaloupe and peach 0.645497224368\n",
      "cosine similarity between cantaloupe and Capizzi 0.348155311911\n",
      "cosine similarity between cantaloupe and Phoenix 0.436435780472\n",
      "cosine similarity between peach and Capizzi 0.404519917478\n",
      "cosine similarity between peach and Phoenix 0.507092552837\n",
      "cosine similarity between Capizzi and Phoenix 0.341881729379\n"
     ]
    }
   ],
   "source": [
    "# reset the generator\n",
    "all_combinations = combinations(vocabulary_plus_oov, 2)\n",
    "# iterate through all words\n",
    "for (word1, word2) in all_combinations:\n",
    "    spelling_vector_1 = make_spelling_vector(word1)\n",
    "    spelling_vector_2 = make_spelling_vector(word2)\n",
    "    print(\"cosine similarity between {} and {}\".format(word1, word2), cos_sim(spelling_vector_1, spelling_vector_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've successfully generated similarity scores!  But...\n",
    "\n",
    "Do they really reflect anything semantic?  In other words, does it make sense that \"peach\" and \"Phoenix\" (`cosine similarity = 0.507`) are more similar than \"peach\" and \"orange\" (`cosine similarity = .365`)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO load word to vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO method for evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
